{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "from statistics import *\n",
    "# Import matplotlib to visualize the model\n",
    "import matplotlib.pyplot as plt\n",
    "# Seaborn is a Python data visualization library based on matplotlib\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# Important packages for the project.\n",
    "from itertools import chain\n",
    "from sklearn import preprocessing, ensemble \n",
    "from sklearn.preprocessing import StandardScaler # Standardisation function from scikit-learn\n",
    "from sklearn.neighbors import LocalOutlierFactor # LocalOutlierFactor function from the scikit-learn\n",
    "from numpy import array\n",
    "from sklearn.model_selection import KFold # KFold Cross-validation function from scikit-learn model selection\n",
    "from sklearn.metrics import accuracy_score # Accuracy score to compute accuracy metric from scikit-learn \n",
    "from sklearn.linear_model import LogisticRegression # logistic regression model from scikit-learn \n",
    "from sklearn.tree import DecisionTreeClassifier # Decision Tree classifier model from scikit-learn\n",
    "from sklearn.svm import SVC # Support Vector Machine classifier model from scikit-learn\n",
    "from sklearn.ensemble import AdaBoostClassifier # AdaBoost classifier model from scikit-learn\n",
    "from sklearn.ensemble import BaggingClassifier # Bagging classifier model from scikit-learn\n",
    "from sklearn.ensemble import RandomForestClassifier # Random forest classifier model from scikit-learn\n",
    "from sklearn.preprocessing import LabelEncoder # Encoding categorical variable into numerical variable\n",
    "from sklearn.model_selection import train_test_split # random Train test split from scikit-learn \n",
    "from sklearn.metrics import classification_report # classification report from scikit-learn\n",
    "from sklearn.metrics import confusion_matrix # confusion matrix between prediction and true value from scikit learn\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "pd.options.display.max_columns = 999\n",
    "os.chdir(os.getcwd())\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is a subset of the DeepSolar database, a solar installation database for the US, built by extracting information\n",
    "from satellite images. Photovoltaic panel installations are identified from over one billion image tiles covering all urban\n",
    "areas as well as locations in the US by means of an advanced machine learning framework. Each image tile records the\n",
    "amount of solar panel systems (in terms of panel surface and number of solar panels) and is complemented with features\n",
    "describing social, economic, environmental, geographical, and meteorological aspects. As such, the database can be\n",
    "employed to relate key environmental, weather and socioeconomic factors with the adoption of solar photovoltaics\n",
    "energy production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset data_project_deepsolar.csv contains a subset of the DeepSolar database. Each row of the dataset is\n",
    "a “tile” of interest, that is an area corresponding to a detected solar power system, constituted by a set of solar panels\n",
    "on top of a building or at a single location such as a solar farm. For each system, a collection of features record social,\n",
    "economic, environmental, geographical, and meteorological aspects of the tile (area) in which the system has been\n",
    "detected. Information about the features are in the file data_project_deepsolar_info.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the datasets \n",
    "Deepsolardata = #Write your code here \n",
    "Deepsolardata_meta = #Write your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse Information about the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deepsolardata_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information about the structure of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Deepsolardata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check for the null values in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the predictor features and target feature\n",
    "X=#Put your code here\n",
    "Y=#Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stastistical behavior of predictor features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out numerical parameters from the X predictor features \n",
    "numerics = ['int64', 'float64']\n",
    "X_numeric = X.select_dtypes(include=numerics)\n",
    "X_numeric_stats=X_numeric.describe()\n",
    "X_numeric_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of reduced numerical features\n",
    "X_numeric.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of correlation matrix\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "#write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove highly correlated predictor variables from the numerical X features and take out the significant features for the classification\n",
    "# Create correlation matrix\n",
    "X_Significant = pd.DataFrame(X_numeric)\n",
    "corr_matrix = X_Significant.corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),\n",
    "                                      k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select significant variables by removing one set of higly correlated variables from thr predictor features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop highly correlated features\n",
    "X_Significant #write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locate outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the outliers in significant \n",
    "lof = #write your code here \n",
    "yhat = #write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape of non outlier rows\n",
    "mask = yhat != -1\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting non-standardised numerical data points to standardised data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise the significant features data points\n",
    "mm_scaler = #Write your code here\n",
    "X_Standardised= mm_scaler.fit_transform(X_Significant.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating standardised dataset \n",
    "X_Standardised=pd.DataFrame(X_Standardised,columns=X_Significant.columns)\n",
    "X_Standardised.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting object type features into categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting object type features into categorical features\n",
    "X[\"state\"] = #Write your code here\n",
    "Y=#Write your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset for target variable\n",
    "Y=pd.DataFrame(Y,columns=['solar_system_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Encoding labels of categorical features into numerical codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating instance of labelencoder\n",
    "labelencoder = #Write your code here\n",
    "# Assigning numerical values and storing in another column\n",
    "Y['solar_system_count_encoded'] = #Write your code here\n",
    "# Encoded target feature\n",
    "YEncoded=Y.solar_system_count_encoded.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded state feature of predictor feature\n",
    "X_Standardised['state']=labelencoder.fit_transform(X[\"state\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting training and testing predictor and target features\n",
    "X_train, X_Test, y_train, y_Test = #Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression model instance\n",
    "logmodel = #Write your code here\n",
    "logmodel.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted target features\n",
    "predictions = #Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_Test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True positive, true nigative, false positive (type-1 error), false negative (type-2 error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tn, fp, fn, tp = confusion_matrix(y_Test,predictions).ravel()\n",
    "tn, fp, fn, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model selection and optimisation of performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. We use k-fold as the cross-validation of different models over training and validation splits, then analyse the generalised performance over test split. This method provides us to establish the best model for the respective data and also help us to determine the number of counts of any selected best model over different random samples of respective data. It helps us to understand the real-time generalised performance of the model in the production pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the given dataset\n",
    "XData = X_Standardised.iloc[:15000,::].values\n",
    "X_test=X_Standardised.iloc[15000:17000,::].values\n",
    "YData=YEncoded[:15000]\n",
    "Y_test=YEncoded[15000:17000]\n",
    "\n",
    "# prepare cross validation\n",
    "acc={'RF': [], 'LR': [],'CT':[],'SVM':[],'ADA':[],'BAG':[]} # dict to record validation accuracy \n",
    "score = {'RF GA': [], 'LR GA': [], 'CT GA':[],'SVM GA':[],'ADA GA':[],'BAG GA':[]} #dict to record genralisation accuracy score\n",
    "\n",
    "df_best_model=pd.DataFrame()\n",
    "# enumerate splits\n",
    "folds=range(2,10) #range of folds\n",
    "for k in folds:\n",
    "    kfold = #Write your code here #k-fold instance\n",
    "    for train, val in #Write your code here:\n",
    "        X_train,X_val=XData[train],XData[val] #K-fold split X data\n",
    "        Y_train,Y_val=YData[train], YData[val] #K-fold split Y data\n",
    "        # logistic regression classifcation object  \n",
    "        logmodel = LogisticRegression(max_iter=10000)\n",
    "        logmodel.fit(X_train,Y_train) #logistic regression fit \n",
    "        # predict target feautures over validation split\n",
    "        val_predictions = logmodel.predict(X_val)\n",
    "        #confusion matrix for logistic model\n",
    "        tn, fp, fn, tp = confusion_matrix(Y_val,val_predictions).ravel()\n",
    "        #calculating accuracy over validation split\n",
    "        logacc=((tn+tp)/(fn+fp+tp+tn))\n",
    "        acc['LR'].insert(0,logacc)\n",
    "        # Random forest classification object\n",
    "        rfmodel = RandomForestClassifier(random_state=1)\n",
    "        rfmodel.fit(X_train, Y_train) #Random forest model fit\n",
    "        # evaluate model \n",
    "        rfprediction = rfmodel.predict(X_val)\n",
    "        rtn, rfp, rfn, rtp = confusion_matrix(Y_val,rfprediction).ravel()\n",
    "        #calculating accuracy over validation split\n",
    "        rfacc=((rtn+rtp)/(rfn+rfp+rtp+rtn))\n",
    "        acc['RF'].insert(0,rfacc)\n",
    "        # Create Decision Tree classifer object\n",
    "        clf = DecisionTreeClassifier()\n",
    "        # Train Decision Tree Classifer\n",
    "        clf = clf.fit(X_train,Y_train)\n",
    "        #Predict the response for validation dataset\n",
    "        clf_prediction = clf.predict(X_val)\n",
    "        clftn, clffp, clffn, clftp = confusion_matrix(Y_val,clf_prediction).ravel()\n",
    "        #calculating validation accuracy\n",
    "        clfacc=((clftn+clftp)/(clffn+clffp+clftp+clftn))\n",
    "        acc['CT'].insert(0,clfacc)\n",
    "        #Create a svm Classifier\n",
    "        SVM = SVC(kernel='linear') # Linear Kernel\n",
    "        #Train the model using the training sets\n",
    "        SVM.fit(X_train, Y_train)\n",
    "        #Predict the response for test dataset\n",
    "        SVM_prediction = SVM.predict(X_val)\n",
    "        svmtn, svmfp, svmfn, svmtp = confusion_matrix(Y_val,SVM_prediction).ravel()\n",
    "        #calculating validation accuracy\n",
    "        svmacc=((svmtn+svmtp)/(svmfn+svmfp+svmtp+svmtn))\n",
    "        acc['SVM'].insert(0,svmacc)\n",
    "        #AdaBoost classifier object \n",
    "        AdaBoost = AdaBoostClassifier(n_estimators=10, random_state=0).fit(X_train, Y_train)\n",
    "        #Predict the response for test dataset\n",
    "        AdaBoost_prediction = AdaBoost.predict(X_val)\n",
    "        adatn, adafp, adafn, adatp = confusion_matrix(Y_val,AdaBoost_prediction).ravel()\n",
    "        #calculating validation accuracy \n",
    "        adaacc=((adatn+adatp)/(adafn+adafp+adatp+adatn))\n",
    "        acc['ADA'].insert(0,adaacc)\n",
    "        #Bagging classifier object \n",
    "        bag = BaggingClassifier(base_estimator=SVC(),n_estimators=10, random_state=0).fit(X_train, Y_train)\n",
    "        bag_prediction=bag.predict(X_val)\n",
    "        bagtn, bagfp, bagfn, bagtp = confusion_matrix(Y_val,bag_prediction).ravel()\n",
    "        #Calculating validation accuracy\n",
    "        bagacc=((bagtn+bagtp)/(bagtn+bagfp+bagfn+bagtp))\n",
    "        acc['BAG'].insert(0,bagacc)\n",
    "        Keymax = # Write your code here \n",
    "        \n",
    "        # Selected model Generalised performance \n",
    "        # Condition of Best classifier = Random forest\n",
    "        if # Write your code here :\n",
    "            rfgeneralprediction = rfmodel.predict(X_test)\n",
    "            grtn, grfp, grfn, grtp = confusion_matrix(Y_test,rfgeneralprediction).ravel()\n",
    "            rfgenralisation=((grtn+grtp)/(grfn+grfp+grtp+grtn))\n",
    "            score['RF GA'].append(rfgenralisation)\n",
    "        # Condition of Best classifier = Logistic Regression \n",
    "        elif #Write your code here:\n",
    "            general_predictions = logmodel.predict(X_test)\n",
    "            gtn, gfp, gfn, gtp = confusion_matrix(Y_test,general_predictions).ravel()\n",
    "            general_logacc=((gtn+gtp)/(gfn+gfp+gtp+gtn))\n",
    "            score['LR GA'].append(general_logacc)\n",
    "        #Condition of Best classifire = Classification tree\n",
    "        elif #Write your code here:\n",
    "            ctgeneral_predictions = clf.predict(X_test)\n",
    "            cttn, ctfp, ctfn, cttp = confusion_matrix(Y_test,ctgeneral_predictions).ravel()\n",
    "            general_ctacc=((cttn+cttp)/(ctfn+ctfp+cttp+cttn))\n",
    "            score['CT GA'].append(general_ctacc)\n",
    "        #Condition of Best Classifier = Support Vector Machine\n",
    "        elif #write your code here:\n",
    "            svmgeneral_predictions = SVM.predict(X_test)\n",
    "            svmgtn, svmgfp, svmgfn, svmgtp = confusion_matrix(Y_test,svmgeneral_predictions).ravel()\n",
    "            general_svmacc=((svmgtn+svmgtp)/(svmgfn+svmgfp+svmgtp+svmgtn))\n",
    "            score['SVM GA'].append(general_svmacc)\n",
    "        #Condition of Best Classifier = AdaBoost\n",
    "        elif #Write your code here:\n",
    "            adageneral_predictions = AdaBoost.predict(X_test)\n",
    "            adagtn, adagfp, adagfn, adagtp = confusion_matrix(Y_test,adageneral_predictions).ravel()\n",
    "            general_adaacc=((adagtn+adagtp)/(adagfn+adagfp+adagtp+adagtn))\n",
    "            score['ADA GA'].append(general_adaacc)\n",
    "        #Condition of Best Classifier = Bagging model\n",
    "        elif #Write your code here:\n",
    "            baggeneral_predictions = bag.predict(X_test)\n",
    "            baggtn, baggfp, baggfn, baggtp = confusion_matrix(Y_test,baggeneral_predictions).ravel()\n",
    "            general_bagacc=((baggtn+baggtp)/(baggfn+baggfp+baggtp+baggtn))\n",
    "            score['BAG GA'].append(general_bagacc)\n",
    "#creating datasets of recorded generalised performance of selected best classifier in each fold\n",
    "df  = pd.DataFrame([score.values()], columns=score.keys())\n",
    "df_best_model = pd.concat([df_best_model, df], axis =1)\n",
    "#creating dataset of recorded validation accuracy of classifiers in each fold\n",
    "df_val  = pd.DataFrame([acc.values()], columns=acc.keys())\n",
    "print('Process completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance analysis of k-fold cv models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot for validation accuracy\n",
    "fig, axs = plt.subplots(ncols=6,figsize=(20,6))\n",
    "sns.distplot(df_val.iloc[0,0],hist=True, kde=True, ax=axs[0]).set_title('RF validation accuracy')\n",
    "sns.distplot(df_val.iloc[0,1], ax=axs[1]).set_title('LR validation accuracy')\n",
    "sns.distplot(df_val.iloc[0,2], ax=axs[2]).set_title('CT validation accuracy')\n",
    "sns.distplot(df_val.iloc[0,3], ax=axs[3]).set_title('SVM validation accuracy')\n",
    "sns.distplot(df_val.iloc[0,4], ax=axs[4]).set_title('AdaBoost validation accuracy')\n",
    "sns.distplot(df_val.iloc[0,5], ax=axs[5]).set_title('Bagging validation accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best classifier count')\n",
    "print('RF selection count',len(df_best_model.iloc[0,0]))\n",
    "print('LR selection connt',len(df_best_model.iloc[0,1]))\n",
    "print('CT selection connt',len(df_best_model.iloc[0,2]))\n",
    "print('SVM selection connt',len(df_best_model.iloc[0,3]))\n",
    "print('AdaBoost selection connt',len(df_best_model.iloc[0,4]))\n",
    "print('Bagging selection connt',len(df_best_model.iloc[0,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2,figsize=(8,4))\n",
    "sns.boxplot(df_best_model.iloc[0,0], ax=axs[0]).set_title('RF Generalisation accuracy boxplot')\n",
    "sns.stripplot(df_best_model.iloc[0,0], \n",
    "                   jitter=True,\n",
    "                   dodge=True, \n",
    "                   marker='o', \n",
    "                   alpha=0.5,\n",
    "                   color='red',ax=axs[0])\n",
    "sns.distplot(df_best_model.iloc[0,0], ax=axs[1]).set_title('RF Generalisation accuracy density plot')\n",
    "plt.axvline(mean(df_best_model.iloc[0,0]), linestyle=\"--\",color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2,figsize=(8,4))\n",
    "sns.boxplot(df_best_model.iloc[0,5], ax=axs[0]).set_title(\"Bagging Generalisation accuracy boxplot\")\n",
    "sns.stripplot(df_best_model.iloc[0,5], \n",
    "                   jitter=True,\n",
    "                   dodge=True, \n",
    "                   marker='o', \n",
    "                   alpha=0.5,\n",
    "                   color='red',ax=axs[0])\n",
    "sns.distplot(df_best_model.iloc[0,5], ax=axs[1]).set_title('Bagging Generalisation accuracy density plot')\n",
    "plt.axvline(mean(df_best_model.iloc[0,5]), linestyle=\"--\",color='red')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
